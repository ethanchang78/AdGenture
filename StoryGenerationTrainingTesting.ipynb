{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction generation"
      ],
      "metadata": {
        "id": "wvbybLHrN4ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm8FGNYwTKQM",
        "outputId": "d1f32e5c-03c9-416d-a3b0-d2bffe4cf859"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "EWFXAr32OOOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Iliasselyaa/CMUBookSummaryDataset\")"
      ],
      "metadata": {
        "id": "7vYTJeBDnA24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, BertTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "48cAa92iQNiV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class and Preprocessing"
      ],
      "metadata": {
        "id": "SYZr-YvLPOwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer should match model being used aka the generator\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "XWezspQLI7Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def text_transform(text):\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\", padding=\"max_length\", max_length=1024, truncation=True)\n",
        "  return input_ids\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    global tokenizer\n",
        "    from transformers import GPT2Tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, dataset, text_transform=None):\n",
        "    self.data = dataset\n",
        "    self.text_transform = text_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    summary = self.data[idx]['summary']\n",
        "    if self.text_transform:\n",
        "        transformed_output = self.text_transform(summary).squeeze(0)\n",
        "    return {'input_ids': transformed_output}\n",
        "\n",
        "train_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "\n",
        "# Access the split datasets\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "val_dataset = train_test_split[\"test\"]\n",
        "\n",
        "train_dataset = MyDataset(train_dataset, text_transform=text_transform)\n",
        "\n",
        "val_dataset = MyDataset(val_dataset, text_transform=text_transform)"
      ],
      "metadata": {
        "id": "xk5sOQl3PVUS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn)\n",
        "\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn)"
      ],
      "metadata": {
        "id": "gXdJRN6iPX9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bcc44a-c87f-43fd-bcb0-6d179719433a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Summary"
      ],
      "metadata": {
        "id": "I6QtRR0As0eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset meant for model that is only trained on last 20% of summary inputs in order to make conclusions\n",
        "\n",
        "class Conclusionizer(Dataset):\n",
        "  def __init__(self, dataset, text_transform=None):\n",
        "    self.data = dataset\n",
        "    self.text_transform = text_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    summary = self.data[idx]['summary']\n",
        "    ending = int(len(summary)*0.8) # skip first 20% of summary\n",
        "    summary = summary[ending:]\n",
        "    if self.text_transform:\n",
        "        transformed_output = self.text_transform(summary).squeeze(0)\n",
        "    return {'input_ids': transformed_output}\n",
        "\n",
        "train_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "\n",
        "# Access the split datasets\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "val_dataset = train_test_split[\"test\"]\n",
        "\n",
        "train_dataset_conclusion = Conclusionizer(train_dataset, text_transform=text_transform)\n",
        "\n",
        "val_dataset_conclusion = Conclusionizer(val_dataset, text_transform=text_transform)"
      ],
      "metadata": {
        "id": "eeKJFdfysuaS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader_conclusion = DataLoader(train_dataset_conclusion, batch_size=1, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn)\n",
        "\n",
        "val_data_loader_conclusion = DataLoader(val_dataset_conclusion, batch_size=1, shuffle=True, num_workers=3, worker_init_fn=worker_init_fn)"
      ],
      "metadata": {
        "id": "SBaJ9tUOuTXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e659f58-1fec-4c46-c8d8-476275ee4a29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Dataloader"
      ],
      "metadata": {
        "id": "gXCIJFbMPWLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    i = 0\n",
        "    for batch in enumerate(train_data_loader):\n",
        "        print(f\"Batch {i} loaded successfully\")\n",
        "        i += 1\n",
        "except Exception as e:\n",
        "    print(f\"Error during loading: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rkxp93ut96uk",
        "outputId": "41f8b346-9c1b-4a2a-a732-56dd43454849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 loaded successfully\n",
            "Batch 1 loaded successfully\n",
            "Batch 2 loaded successfully\n",
            "Batch 3 loaded successfully\n",
            "Batch 4 loaded successfully\n",
            "Batch 5 loaded successfully\n",
            "Batch 6 loaded successfully\n",
            "Batch 7 loaded successfully\n",
            "Batch 8 loaded successfully\n",
            "Batch 9 loaded successfully\n",
            "Batch 10 loaded successfully\n",
            "Batch 11 loaded successfully\n",
            "Batch 12 loaded successfully\n",
            "Batch 13 loaded successfully\n",
            "Batch 14 loaded successfully\n",
            "Batch 15 loaded successfully\n",
            "Batch 16 loaded successfully\n",
            "Batch 17 loaded successfully\n",
            "Batch 18 loaded successfully\n",
            "Batch 19 loaded successfully\n",
            "Batch 20 loaded successfully\n",
            "Batch 21 loaded successfully\n",
            "Batch 22 loaded successfully\n",
            "Batch 23 loaded successfully\n",
            "Batch 24 loaded successfully\n",
            "Batch 25 loaded successfully\n",
            "Batch 26 loaded successfully\n",
            "Batch 27 loaded successfully\n",
            "Batch 28 loaded successfully\n",
            "Batch 29 loaded successfully\n",
            "Batch 30 loaded successfully\n",
            "Batch 31 loaded successfully\n",
            "Batch 32 loaded successfully\n",
            "Batch 33 loaded successfully\n",
            "Batch 34 loaded successfully\n",
            "Batch 35 loaded successfully\n",
            "Batch 36 loaded successfully\n",
            "Batch 37 loaded successfully\n",
            "Batch 38 loaded successfully\n",
            "Batch 39 loaded successfully\n",
            "Batch 40 loaded successfully\n",
            "Batch 41 loaded successfully\n",
            "Batch 42 loaded successfully\n",
            "Batch 43 loaded successfully\n",
            "Batch 44 loaded successfully\n",
            "Batch 45 loaded successfully\n",
            "Batch 46 loaded successfully\n",
            "Batch 47 loaded successfully\n",
            "Batch 48 loaded successfully\n",
            "Batch 49 loaded successfully\n",
            "Batch 50 loaded successfully\n",
            "Batch 51 loaded successfully\n",
            "Batch 52 loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 53 loaded successfully\n",
            "Batch 54 loaded successfully\n",
            "Batch 55 loaded successfully\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-67bc62334781>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {i} loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Novelizer"
      ],
      "metadata": {
        "id": "cHSoI4ocPY7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Novelizer(nn.Module):\n",
        "  def __init__(self, generator, device):\n",
        "    super(Novelizer, self).__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.generator = generator\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, input_ids, labels):\n",
        "    input_ids = input_ids.to(self.device)\n",
        "\n",
        "    outputs = self.generator(input_ids=input_ids, labels=input_ids)\n",
        "    return outputs\n",
        "\n",
        "  def generate_story_intro(self, story_type, num_return_sequences=1, device='cpu'): #Change num of sequences to get different answers\n",
        "    # Tokenize the story type prompt\n",
        "    input_ids = self.tokenizer.encode(story_type, return_tensors=\"pt\").to(device)\n",
        "    num_beams = max(num_return_sequences, 1)\n",
        "    # Use the forward method to generate text\n",
        "    self.generator.to(device)\n",
        "    generated_sequences = self.generator.generate(\n",
        "            input_ids,\n",
        "            max_length=64, # max length\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            num_beams=num_beams,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    return [\n",
        "            self.tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
        "            for generated_sequence in generated_sequences\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "yf5bv-0_OJ_V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "iXhAXZdvPbBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cYISFGhbGifL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOT THIS ONE"
      ],
      "metadata": {
        "id": "WBmumMKHWCzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for when you need to load the latest model\n",
        "model = Novelizer(generator=GPT2LMHeadModel.from_pretrained('gpt2'), device=device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Junior/ACV/model6.pth')) # loading model 6"
      ],
      "metadata": {
        "id": "qCZls1unjFXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# where i set the generator\n",
        "# model = Novelizer(generator=GPT2LMHeadModel.from_pretrained('gpt2'), device=device) # pretrained gpt2 for now\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=3e-4) # optimizer and learning rate\n",
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=1, device='cpu'):\n",
        "  model.to(device)\n",
        "\n",
        "  accumulation_steps = 4  # Adjust based on your needs\n",
        "  optimizer.zero_grad()\n",
        "  patience = 0\n",
        "  best_val_loss = 100000\n",
        "  for epoch in range(num_epochs):\n",
        "    # initial training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    c = 0\n",
        "    for batch in train_loader:\n",
        "      if c % 1000 == 0:\n",
        "        print(c)\n",
        "      c += 1\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, labels=input_ids)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      # Backward pass and optimize\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience = 0\n",
        "        # model checkpoint\n",
        "        model_save_path = '/content/drive/MyDrive/Junior/ACV/model6.pth' # now saving model 3\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "    else:\n",
        "        patience += 1\n",
        "    if patience > 3:\n",
        "        break  # e arly stopping trigger\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "kW7oreFpOW35"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "17xfxNHc6zbb",
        "outputId": "0f2d130d-b9fa-49f8-84be-7c46f6673a51"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   | 486874 KiB | 486874 KiB | 486874 KiB |\\n|       from large pool |      0 B   | 486400 KiB | 486400 KiB | 486400 KiB |\\n|       from small pool |      0 B   |    474 KiB |    474 KiB |    474 KiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   | 486874 KiB | 486874 KiB | 486874 KiB |\\n|       from large pool |      0 B   | 486400 KiB | 486400 KiB | 486400 KiB |\\n|       from small pool |      0 B   |    474 KiB |    474 KiB |    474 KiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   | 486093 KiB | 486093 KiB | 486093 KiB |\\n|       from large pool |      0 B   | 485619 KiB | 485619 KiB | 485619 KiB |\\n|       from small pool |      0 B   |    474 KiB |    474 KiB |    474 KiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   | 542720 KiB | 542720 KiB | 542720 KiB |\\n|       from large pool |      0 B   | 540672 KiB | 540672 KiB | 540672 KiB |\\n|       from small pool |      0 B   |   2048 KiB |   2048 KiB |   2048 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |  77386 KiB | 449236 KiB | 449236 KiB |\\n|       from large pool |      0 B   |  75776 KiB | 446720 KiB | 446720 KiB |\\n|       from small pool |      0 B   |   2045 KiB |   2516 KiB |   2516 KiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |     148    |     148    |     148    |\\n|       from large pool |       0    |      50    |      50    |      50    |\\n|       from small pool |       0    |      98    |      98    |      98    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |     148    |     148    |     148    |\\n|       from large pool |       0    |      50    |      50    |      50    |\\n|       from small pool |       0    |      98    |      98    |      98    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |      21    |      21    |      21    |\\n|       from large pool |       0    |      20    |      20    |      20    |\\n|       from small pool |       0    |       1    |       1    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |      23    |      39    |      39    |\\n|       from large pool |       0    |      21    |      37    |      37    |\\n|       from small pool |       0    |       2    |       2    |       2    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Z4vlAyW9F12v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_data_loader, val_data_loader, optimizer, criterion, scheduler, num_epochs=100, device=device)"
      ],
      "metadata": {
        "id": "RUTLhl-8kEjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "TgBq8nb4Pcwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### STOP #######\n",
        "import torch\n",
        "model_save_path = '/content/drive/MyDrive/Junior/ACV/conclusionizer3.pth' # now saving model 3\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "uPUL4cOLPdj_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate_story_intro('Then cheryl left on a boat to moscow',num_return_sequences=2, device=device)"
      ],
      "metadata": {
        "id": "oDs0Bm9xBC2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}